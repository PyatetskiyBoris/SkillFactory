{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pandas import Series\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import balanced_accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функции, использующиеся при обработке данных\n\n\n# Функция для расчёта статистических параметров: iqr, квартили, границы выбросов, количество выбросов\n# Передаём на вход столбец DataFrame\ndef quantile_info(column):\n    # Перцентили и IQR\n    perc25 = column.quantile(0.25)\n    perc75 = column.quantile(0.75)\n    iqr = perc75 - perc25\n    # Границы выбросов\n    low = perc25 - 1.5*iqr\n    high = perc75 + 1.5*iqr\n    # Количество выбросов в столбце\n    count_low = (column[(column<low)]).count()\n    count_high = (column[(column>high)]).count()\n    # Печатаем вычисленные значения на экране    \n    print(\n        f'25-й перцентиль: {perc25}\\n',\n        f'75-й перцентиль: {perc75}\\n',\n        f'IQR: {iqr}\\n',\n        f'Границы выбросов: [{low}, {high}]\\n',\n        f'Количество выбросов вниз: {count_low}\\n',\n        f'Количество выбросов вверх: {count_high}',\n        sep='')\n\n    \n# Функция для замены выбросов медианным значением\ndef change_on_median(column):\n    # Перцентили и IQR\n    perc25 = column.quantile(0.25)\n    perc50 = column.quantile(0.5)\n    perc75 = column.quantile(0.75)\n    iqr = perc75 - perc25\n    # Границы выбросов\n    low = perc25 - 1.5*iqr\n    high = perc75 + 1.5*iqr\n    \n    return column.apply(lambda x: perc50 if (x < low) | (x > high) else x)\n\n\n# Функция, разделяющая датасет на обучающую и валидационную части со стандартизацией числовых признаков\n# Принимает на вход DataFrame, возвращает 4 массива numpy\ndef form(DataFrame):\n    y = DataFrame['default'].values\n    x = DataFrame.drop(['default'], axis=1)\n    \n    x_train_pd, x_valid_pd, y_train, y_valid = train_test_split(x, y, test_size=0.20, random_state=42)\n    \n    # Cтандартизируем числовые признаки\n    scaler = StandardScaler()\n    x_train_num = scaler.fit_transform(x_train_pd[num_cols].values)\n    x_valid_num = scaler.transform(x_valid_pd[num_cols].values)\n    \n    # И соберём все признаки в numpy array\n    x_train = np.hstack([x_train_num, x_train_pd.drop(columns=num_cols).values])\n    x_valid = np.hstack([x_valid_num, x_valid_pd.drop(columns=num_cols).values])\n    \n    return x_train, x_valid, y_train, y_valid\n\n\n# Функция, выводящая на экран график ROC_AUC\ndef print_roc_auc(val, pred):\n    fpr, tpr, threshold = roc_curve(val, pred)\n    roc_auc = roc_auc_score(val, pred)\n    \n    plt.figure()\n    plt.plot([0, 1], label='Baseline', linestyle='--')\n    plt.plot(fpr, tpr, label = 'Regression')\n    plt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.legend(loc = 'lower right')\n    plt.show()\n    \n    \n# Функция, выводящая на экран значения метрик логистической регрессии\ndef print_metrics(val, pred, acc=0):\n    # Выбираем, печатать ли метрику accuracy\n    if acc == 1:\n        print('accuracy:', '%0.4f' %accuracy_score(val, pred))\n        \n    print('balanced_accuracy:', '%0.4f' %balanced_accuracy_score(val, pred))\n    print('precision_score:', '%0.4f' %precision_score(val, pred))\n    print('recall_score:', '%0.4f' %recall_score(val, pred))\n    print('f1_score:', '%0.4f' %f1_score(val, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка начальных данных","metadata":{}},{"cell_type":"code","source":"# Загружаем таблицу c данными клиентов банка\nDATA_DIR = '/kaggle/input/sf-dst-scoring/'\n# Загружаем данные для обучения модели\ntrain = pd.read_csv(DATA_DIR+'/train.csv')\n# Загружаем данные для теста модели\ntest = pd.read_csv(DATA_DIR+'/test.csv')\n# Загружаем файл с итоговыми данными\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# описание данных\n\n# client_id - идентификатор клиента\n# education - уровень образования\n# sex - пол заемщика\n# age - возраст заемщика\n# car - флаг наличия автомобиля\n# car_type - флаг автомобиля иномарки\n# deсline_app_cnt - количество отказанных прошлых заявок\n# good_work - флаг наличия “хорошей” работы\n# bki_request_cnt - количество запросов в БКИ\n# home_address - категоризатор домашнего адреса\n# work_address - категоризатор рабочего адреса\n# income - доход заемщика\n# foreign_passport - наличие загранпаспорта\n# sna - связь заемщика с клиентами банка\n# first_time - давность наличия информации о заемщике\n# score_bki - скоринговый балл по данным из БКИ\n# region_rating - рейтинг региона\n# app_date - дата подачи заявки\n# default - флаг дефолта по кредиту","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Смотрим исходные данные\ndisplay(train.head(10))\ndisplay(train.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(test.head(10))\ndisplay(test.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(sample_submission.head(10))\ndisplay(sample_submission.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для корректной обработки признаков объединяем обучающую и тестовую выборки в один датасет\ntrain['sample'] = 1 # помечаем, где обучающая выборка\ntest['sample'] = 0 # помечаем, где тестовая выборка\n# в тесте у нас нет значения default, мы его должны предсказать, \n# поэтому пока заполним произвольными значениями.\n# Пусть каждый седьмой клиент имеет default=1\ntest['default'] = test['client_id'].apply(lambda x: 1 if x%7 == 0 else 0) \n\ndata = test.append(train, sort=False).reset_index(drop=True) # объединяем","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Информация по объединённому датасету\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь на основе этих данных построим наивную модель логистической регрессии, для понимания, какого результата можно добиться с минимальной обработкой данных.","metadata":{}},{"cell_type":"markdown","source":"## Обработка данных для наивной модели","metadata":{}},{"cell_type":"code","source":"# Скопируем датасет\ndata_naiv = data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# В первую очередь посмотрим распределение целевой переменной default в обучающей выборке:\nlabels = train.default.value_counts().index\nvalues = train.default.value_counts().values\n\nplt.title('Разбивка клиентов по \"default\"')\nplt.pie(values, labels=labels, explode=[0.3, 0], autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видно, клиентов с дефолтом значительно меньше, чем клиентов без дефолта. Поэтому в дальнейшем при обучении модели нужно будет применять методы для несбалансированных классов.","metadata":{}},{"cell_type":"code","source":"# Смотрим на количество пропусков по столбцам\ndata_naiv.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Как мы видим, пропуски встречаются только в информации об образовании клиентов.\n# Посмотрим на признак поближе.\ndata_naiv['education'].value_counts(dropna=False).plot.barh()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что значат уровни образования:\n\nSCH - school - те, у кого среднее образование (только школа).\n\nUGR - undergraduate - бакалавры.\n\nGRD - магистры\n\nPGR - postgraduate - учёная степень PhD (кандидаты наук)\n\nACD - высший уровень.\n\nПропусков немного относительно общего количества записей (~0.4%), поэтому присоединим их \nк самой многочисленной категории SCH.","metadata":{}},{"cell_type":"code","source":"data_naiv['education'] = data_naiv['education'].fillna('SCH')\n# Проверяем количество пропусков по столбцам\ndata_naiv.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Категориальные и числовые признаки обрабатываются по-разному.\n# Посмотрим, к каким категориям какие признаки относятся.\nfor a in data_naiv.columns:\n    k = data_naiv[a].nunique()\n    print(f'Количество значений признаков в столбце {a}: {k}')\n    if a == 'sex':\n        print('(В современном безумном мире мы должны проверить)')\n    print(f'Тип значений признаков в столбце {a}: {data_naiv[a].dtypes}\\n\\n')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Таким образом, исходя из типа данных и их описания, мы можем разделить признаки на категории следующим образом:\n\n- категориальные - education, region_rating, home_address, work_address, sna, first_time\n- бинарные - sex, car, car_type, good_work, foreign_passport\n- числовые - age, decline_app_cnt, score_bki, bki_request_cnt, income\n\nОтдельный случай со столбцом app_date, так как там содержится дата. В наивной модели не будем его учитывать","metadata":{}},{"cell_type":"code","source":"# Создадим списки колонок по категориям для удобства работы.\n# Столбцы default и client_id не включаем, так как первый - целевая переменная,\n# а второй - просто номер клиента\nnum_cols = ['age', 'decline_app_cnt', 'score_bki', 'bki_request_cnt', 'income']\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\ncat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\n\n# Удаляем столбец app_date и sample\ndata_naiv = data_naiv.drop(['app_date', 'sample'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для начала переведём бинарные признаки в числа с помощью LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor b in bin_cols:\n    data_naiv[b] = label_encoder.fit_transform(data_naiv[b])\n    \n# убедимся в преобразовании    \ndata_naiv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Посмотрим на распределение числовых переменных:\nfor c in num_cols:\n    data_naiv[c].plot.hist(bins=26)\n    plt.title(c)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"После построения гистограмм стало очевидно, что распределения почти всех числовых переменных имеют тяжёлый правый хвост.\n\nДля того, чтобы сделать распределение данных переменных более нормальным, можно работать с логарифмированными величинами этих переменных. Тогда мы сможем избежать чувствительности к сильным отклонениям в суммах у линейных моделей.","metadata":{}},{"cell_type":"code","source":"# прологарифмируем все числовые столбцы, кроме score_bki\nfor d in num_cols:\n    data_naiv[d] = data_naiv[d].apply(lambda x: np.log(x + 1) if d != 'score_bki' else x)\n    data_naiv[d].plot.hist(bins=26)\n    plt.title(d)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Закодируем категориальные признаки с помощью OneHotEncoding\nX_naiv_cat =  pd.get_dummies(data_naiv, columns=cat_cols).values\nX_naiv_cat.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Объединим стандартизованные числовые, бинарные и закодированные категориальные\n# переменные в одно признаковое пространство, разделив при этом признаки\n# и целевую переменную.\nX_naiv = np.hstack([data_naiv[num_cols].values, data_naiv[bin_cols].values, X_naiv_cat])\nY_naiv = data_naiv['default'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Разделим данные для обучения на тренировочную и тестовую выборки следующим образом:\nX_train_naiv, X_test_naiv, Y_train_naiv, Y_test_naiv = train_test_split(X_naiv, Y_naiv, test_size=0.20, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучаем модель на стандартных настройках логистической регрессии\n# Логистическая регрессия\nmodel_naiv = LogisticRegression(max_iter=5000)\nmodel_naiv.fit(X_train_naiv, Y_train_naiv)\n\nprobs_naiv = model_naiv.predict_proba(X_test_naiv)\nY_pred_naiv = model_naiv.predict(X_test_naiv)\nprobs_naiv = probs_naiv[:,1]\n\n# Выводим на экран график roc_auc\nprint_roc_auc(Y_test_naiv, probs_naiv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Матрица ошибок для нашего алгоритма\nprint(confusion_matrix(Y_test_naiv, Y_pred_naiv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наша наивная модель все примеры причислила к классу default=0. Что вполне закономерно для такого разбаланса классов.\n\nПосмотрим остальные метрики качества для логистической регрессии","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:48:04.740247Z","iopub.execute_input":"2021-11-29T13:48:04.740781Z","iopub.status.idle":"2021-11-29T13:48:04.772762Z","shell.execute_reply.started":"2021-11-29T13:48:04.740731Z","shell.execute_reply":"2021-11-29T13:48:04.772004Z"}}},{"cell_type":"code","source":"print_metrics(Y_test_naiv, Y_pred_naiv, acc=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Так как наша модель не дала значений TN и FN, метрики precision, recall и f1 не посчитались.\nА вот метрика accuracy показала очень хорошее значение, что лишний раз подтверждает, что её нельзя использовать в случае несбалансированных классов. Поэтому в дальнейшем будем смотреть характеристику balanced_accuracy, которая гораздо лучше подходит для таких случаев.","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:49:20.181909Z","iopub.execute_input":"2021-11-29T13:49:20.182536Z","iopub.status.idle":"2021-11-29T13:49:20.202349Z","shell.execute_reply.started":"2021-11-29T13:49:20.182494Z","shell.execute_reply":"2021-11-29T13:49:20.200972Z"}}},{"cell_type":"markdown","source":"Как видим, наивная модель не сильно отличается от случайного угадывания. Для улучшения характеристик модели необходимо проводить исследование и обработку данных.","metadata":{}},{"cell_type":"markdown","source":"## Анализ данных","metadata":{}},{"cell_type":"code","source":"# Вспоним, как выглядит объединённый датасет\ndisplay(data.head(10))\ndisplay(data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Признак client_id**","metadata":{}},{"cell_type":"code","source":"# Посмотрим, сколько значений в этом столбце\ndata['client_id'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Получается, здесь просто порядковый номер клиента. Никакой обработки для этого столбца не предполагается.","metadata":{}},{"cell_type":"markdown","source":"**Признак app_date**","metadata":{}},{"cell_type":"code","source":"# Здесь содержится дата заявки клиента в виде строки. Преобразуем эти данные в формат даты\ndata['app_date'] = pd.to_datetime(data['app_date'])\ndata['app_date'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"На основе даты создадим новые признаки.","metadata":{"execution":{"iopub.status.busy":"2021-11-28T12:08:35.000786Z","iopub.execute_input":"2021-11-28T12:08:35.001456Z","iopub.status.idle":"2021-11-28T12:08:35.010498Z","shell.execute_reply.started":"2021-11-28T12:08:35.001423Z","shell.execute_reply":"2021-11-28T12:08:35.009593Z"}}},{"cell_type":"code","source":"# Посмотрим самую раннюю и самую позднюю даты в этом признаке\nprint(f\"Самая ранняя дата: {data['app_date'].min()}\")\nprint(f\"Самая поздняя дата: {data['app_date'].max()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Все даты в 2014 году. Поэтому признак с годом создавать не будем, создадим следующие признаки:\n\nномер месяца, номер дня недели, номер дня месяца, номер дня года","metadata":{}},{"cell_type":"code","source":"data['month'] = data['app_date'].apply(lambda t: t.month)\ndata['day'] = data['app_date'].apply(lambda t: t.day)\ndata['weekday'] = data['app_date'].apply(lambda t: t.isoweekday())\ndata['yearday'] = data['app_date'].apply(lambda t: t.dayofyear)\n\n# И удалим признак app_date\ndata = data.drop(['app_date'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Добавим вновь созданные признаки в списки колонок для облегчения работы\n# 'weekday' к категориальным\ncat_cols.append('weekday')\n\n# Остальные к числовым\nnum_cols.append('month')\nnum_cols.append('day')\nnum_cols.append('yearday')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Теперь обработаем числовые признаки\nК числовым мы отнесли следующие признаки:\n\nnum_cols = ['age', 'decline_app_cnt', 'score_bki', 'bki_request_cnt', 'income']","metadata":{}},{"cell_type":"markdown","source":"Как мы помним, почти все признаки имеют сильное смещение. Поэтому для нормализации сначала прологарифмируем их, потом будем искать выбросы.","metadata":{}},{"cell_type":"markdown","source":"**Признак age**","metadata":{}},{"cell_type":"code","source":"data['age'] = data['age'].apply(lambda x: np.log(x + 1))\n\ndata['age'].hist(bins=100)\nplt.show()\n\nquantile_info(data['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выбросы отсутствуют.","metadata":{}},{"cell_type":"markdown","source":"**Признак decline_app_cnt**","metadata":{}},{"cell_type":"code","source":"# Сначала посмотрим на значения признака\ndata['decline_app_cnt'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видим, подавляющее количество клиентов не получало отказов по предыдущим заявкам.\nПоэтому на основе этого признака создадим бинарный, показывающий, получал ли человек отказ ранее.","metadata":{}},{"cell_type":"code","source":"data['decline_app_bin'] = data['decline_app_cnt'].apply(lambda g: int(g != 0))\n\n# Добавляем признак в список бинарных\nbin_cols.append('decline_app_bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь создадим ещё один признак, прологарифмировав значения признака.\nСам признак оставим без изменений, потом создадим на его основе ещё пару признаков.","metadata":{}},{"cell_type":"code","source":"data['decline_app_log'] = data['decline_app_cnt'].apply(lambda x: np.log(x + 1))\n\ndata['decline_app_log'].hist(bins=50)\nplt.show()\n\nquantile_info(data['decline_app_log'])\n\n# Добавляем новый признак в список числовых\nnum_cols.append('decline_app_log')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выбросы оставим, потому что иначе в столбце останутся одни нулевые значения","metadata":{}},{"cell_type":"markdown","source":"**Признак bki_request_cnt**","metadata":{}},{"cell_type":"code","source":"# Сначала посмотрим на значения признака\ndata['bki_request_cnt'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Разница в количестве ненулевых и нулевых значений не такая большая, как в признаке decline_app_cnt.\nНо, тем не менее, на основе этого признака создадим бинарный, показывающий, обращался ли человек в БКИ ранее. Может, это будет полезно для нашей модели.","metadata":{}},{"cell_type":"code","source":"data['bki_request_bin'] = data['bki_request_cnt'].apply(lambda h: int(h != 0))\n\n# Добавляем признак в список бинарных\nbin_cols.append('bki_request_bin')\n\ndata['bki_request_bin'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь создадим ещё один признак, прологарифмировав значения признака.\nСам признак оставим без изменений, потом создадим на его основе ещё пару признаков.","metadata":{}},{"cell_type":"code","source":"data['bki_request_log'] = data['bki_request_cnt'].apply(lambda x: np.log(x + 1))\n\ndata['bki_request_log'].hist(bins=50)\nplt.show()\n\nquantile_info(data['bki_request_log'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# В столбце содержится 15 выбросов. Заменим их на меданные значения\ndata['bki_request_log'] = change_on_median(data['bki_request_log'])\n\n# Добавляем новый признак в список числовых\nnum_cols.append('bki_request_log')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Признак score_bki**","metadata":{}},{"cell_type":"code","source":"# Посмотрим на характеристики распределения\ndata['score_bki'].hist(bins=50)\nplt.show()\n\nquantile_info(data['score_bki'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создадим признак, заменив выбросы на медианное значение\ndata['score_bki_clear'] = change_on_median(data['score_bki'])\n\n# Добавляем новый признак в список числовых\nnum_cols.append('score_bki_clear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Признак income**","metadata":{}},{"cell_type":"code","source":"# Прологарифмируем и посмотрим наличие выбросов\ndata['income'] = data['income'].apply(lambda x: np.log(x + 1))\ndata['income'].hist(bins=50)\nplt.show()\n\nquantile_info(data['income'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Заменим все выбросы на медианное значение\ndata['income'] = change_on_median(data['income'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создание признаков из информации в БКИ\ndata['uno'] = (data['bki_request_cnt'] - data['decline_app_cnt']) * data['score_bki']\ndata['uno_log'] = (data['bki_request_log'] - data['decline_app_log']) * data['score_bki_clear']\n\ndata['dos'] = data['bki_request_cnt'] * data['decline_app_cnt'] * data['score_bki']\ndata['dos_log'] = data['bki_request_log'] * data['decline_app_log'] * data['score_bki_clear']\n\n# Добавляем новые признаки в список числовых\nnum_cols.append('uno')\nnum_cols.append('uno_log')\nnum_cols.append('dos')\nnum_cols.append('dos_log')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Теперь обработаем категориальные признаки\nИзначально к категориальным мы отнесли следующие признаки:\n\ncat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']","metadata":{}},{"cell_type":"code","source":"# Посмотрим на них поближе:\ndata[cat_cols].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Как мы помним, в столбце education содержатся пропуски.\n# Заполним их тем же способом, что и для наивной модели.\ndata['education'] = data['education'].fillna('SCH')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Посмотрим распределения категориальных переменных в выборке:\nfor e in cat_cols:\n    data[e].value_counts(dropna=False).plot.barh()\n    plt.title(e)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"К сожалению, без дополнительной информации мы не можем сделать выводов о содержании данных столбцов, поэтому оставим эти данные без обработки. Позже закодируем их с помощью OneHotEncoder.","metadata":{}},{"cell_type":"markdown","source":"### Теперь обработаем бинарные признаки\nК бинарным мы отнесли следующие признаки:\n\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']","metadata":{}},{"cell_type":"code","source":"# Посмотрим на них поближе:\ndata[bin_cols].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Переведём бинарные признаки в числа с помощью LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor f in bin_cols:\n    data[f] = label_encoder.fit_transform(data[f])\n    \n# убедимся в преобразовании    \ndata[bin_cols].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Объединим признаки car и car_type в один, категориальный:\n\n0 - нет машины\n\n1 - есть отечественная машина\n\n2 - есть иномарка","metadata":{}},{"cell_type":"code","source":"data['car'] = data['car'] + data['car_type']\ndata.drop('car_type', axis=1, inplace=True)\n# Удалим признак из бинарных и переведём его в категориальные\nbin_cols.remove('car_type')\nbin_cols.remove('car')\ncat_cols.append('car')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Теперь оценим важность признаков и посмотрим матрицу корреляций","metadata":{"execution":{"iopub.status.busy":"2021-11-28T12:50:44.857093Z","iopub.execute_input":"2021-11-28T12:50:44.857335Z","iopub.status.idle":"2021-11-28T12:50:44.860398Z","shell.execute_reply.started":"2021-11-28T12:50:44.857307Z","shell.execute_reply":"2021-11-28T12:50:44.859822Z"}}},{"cell_type":"markdown","source":"Для оценки значимости категориальных и бинарных переменных будем использовать функцию mutual_info_classif из библиотеки sklearn. mutual_info_classif сначала требует, чтобы категориальные значения были сопоставлены с целочисленными значениями, поэтому необходимо преобразовать столбец education к целочисленным значениям","metadata":{}},{"cell_type":"code","source":"edu_dict = {'ACD':0, 'PGR':1, 'UGR':2, 'GRD':3, 'SCH':4}\ndata['education'] = data['education'].apply(lambda x: edu_dict[x])\ndata['education'].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_cat = Series(mutual_info_classif(data[bin_cols + cat_cols], data['default'],\n                                     discrete_features=True), index = bin_cols + cat_cols)\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:35:48.643753Z","iopub.execute_input":"2021-11-27T14:35:48.643993Z","iopub.status.idle":"2021-11-27T14:35:48.661872Z","shell.execute_reply.started":"2021-11-27T14:35:48.643964Z","shell.execute_reply":"2021-11-27T14:35:48.66031Z"}}},{"cell_type":"code","source":"# Cильная корреляция между переменными вредна для\n# линейных моделей из-за неустойчивости полученных оценок.\n# Оценим корреляцию Пирсона для численных переменных:\nplt.figure(figsize=(14, 8))\nsns.heatmap(data[num_cols].corr().abs(), vmin=0, vmax=1, annot=True)\nplt.show()\n\n# Критерием для удаления признаков будем считать корреляцию выше 0,7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Для оценки значимости числовых переменных будем использовать функцию f_classif из библиотеки sklearn. \n\nВозможности модуля sklearn.feature_selection могут быть использованы не только для выбора важных признаков, но и для уменьшения размерности, улучшения предсказательной силы моделей, либо для повышения их производительности на очень многомерных наборах данных.\n\nВ основе метода оценки значимости переменных лежит однофакторный дисперсионный анализ (ANOVA). Основу процедуры составляет обобщение результатов двух выборочных t-тестов для независимых выборок (2-sample t). \n\nВ качестве меры значимости мы будем использовать значение f-статистики. Чем значение статистики выше, тем меньше вероятность того, что средние значения не отличаются, и тем важнее данный признак для нашей линейной модели.","metadata":{}},{"cell_type":"code","source":"imp_num = pd.Series(f_classif(data[num_cols], data['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"На основании матрицы корреляций и оценки важности признаков удалим из датасета следующие столбцы:\n\nweekday, uno,dos_log, uno_log, day, month, bki_request_cnt, decline_app_cnt, score_bki\n","metadata":{}},{"cell_type":"code","source":"data.drop(['weekday', 'uno', 'dos_log', 'uno_log', 'day', 'month', 'bki_request_cnt', 'decline_app_cnt', 'score_bki'], axis=1, inplace=True)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols.remove('uno')\nnum_cols.remove('uno_log')\nnum_cols.remove('dos_log')\nnum_cols.remove('day')\nnum_cols.remove('month')\nnum_cols.remove('bki_request_cnt')\nnum_cols.remove('decline_app_cnt')\nnum_cols.remove('score_bki')\ncat_cols.remove('weekday')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Закодируем категориальные и бинарные признаки с помощью OneHotEncoding\ndata =  pd.get_dummies(data, columns=cat_cols+bin_cols)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Выделим в выборке обучающую часть и тестовую по признаку sample\ntrain_data = data.query('sample == 1').drop(['sample', 'client_id'], axis=1)\ntest_data = data.query('sample == 0').drop(['sample', 'client_id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим обучающую выборку на обучающую и валидационную части, для проверки разных моделей. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submission на kaggle.\nЗаодно старндартизируем числовые признаки**","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = form(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Модели логистической регрессии","metadata":{}},{"cell_type":"markdown","source":"### Модель логистической регрессии с параметрами по умолчанию","metadata":{}},{"cell_type":"code","source":"# Обучаем модель на стандартных настройках логистической регрессии\nmodel_simple = LogisticRegression(max_iter=5000)\nmodel_simple.fit(X_train, Y_train)\n\nprobs_simple = model_simple.predict_proba(X_valid)\nY_pred_simple = model_simple.predict(X_valid)\nprobs_simple = probs_simple[:,1]\n\n# Выводим на экран график roc_auc\nprint_roc_auc(Y_valid, probs_simple)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_valid, Y_pred_simple)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Как видим, новая модель уже пытается различать классы.\n# Посмотрим остальные метрики качества модели\nprint_metrics(Y_valid, Y_pred_simple)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Метрики качества очень плохи. Вспомним, что у нас несбалансированные классы: значений default=1 гораздо меньше, чем default=0. Построим новую модель с балансировкой классов","metadata":{}},{"cell_type":"markdown","source":"### Модель логистической регрессии с балансировкой классов","metadata":{}},{"cell_type":"code","source":"# Обучаем модель логистической регрессии \nmodel_balance = LogisticRegression(class_weight='balanced', solver='liblinear',max_iter=5000)\nmodel_balance.fit(X_train, Y_train)\nprobs_balance = model_balance.predict_proba(X_valid)\nY_pred_balance = model_balance.predict(X_valid)\n\nprobs_balance = probs_balance[:,1]\n\n\nfpr, tpr, threshold = roc_curve(Y_valid, probs_balance)\nroc_auc = roc_auc_score(Y_valid, probs_balance)\n\n# Выводим на экран график roc_auc\nprint_roc_auc(Y_valid, probs_balance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_valid, Y_pred_balance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Судя по матрице ошибок, новая модель работает лучше.\n# Посмотрим другие метрики качества нашей модели\nprint_metrics(Y_valid, Y_pred_balance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Действительно, метрики качества показали существенный рост.\n\nОднако попробуем другие способы балансировки классов","metadata":{}},{"cell_type":"markdown","source":"# Модель с undersampling","metadata":{}},{"cell_type":"markdown","source":"Сравняем количество классов в обучающей выборке, удалив большое количество примеров с default=0","metadata":{"execution":{"iopub.status.busy":"2021-11-29T16:32:51.930043Z","iopub.execute_input":"2021-11-29T16:32:51.931276Z","iopub.status.idle":"2021-11-29T16:32:51.959038Z","shell.execute_reply.started":"2021-11-29T16:32:51.931136Z","shell.execute_reply":"2021-11-29T16:32:51.957927Z"}}},{"cell_type":"code","source":"data_under_1 = train_data[train_data['default'] == 1].copy()\ncount_1 = len(data_under_1)\ndata_under_0 = train_data[train_data['default'] == 0].iloc[:count_1,:].copy()\ndata_under = pd.concat([data_under_1, data_under_0])\n# Проверим количество классов\ndata_under['default'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Разделим датасет на обучающий и валидационный\nX_train_under, X_valid_under, Y_train_under, Y_valid_under = form(data_under)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучаем модель логистической регрессии\nmodel_under = LogisticRegression(solver='liblinear', max_iter=5000)\nmodel_under.fit(X_train_under, Y_train_under)\n\nprobs_under = model_under.predict_proba(X_valid_under)\nY_pred_under = model_under.predict(X_valid_under)\nprobs_under = probs_under[:,1]\n\n\nfpr, tpr, threshold = roc_curve(Y_valid_under, probs_under)\nroc_auc = roc_auc_score(Y_valid_under, probs_under)\n\n# Выводим на экран график roc_auc\nprint_roc_auc(Y_valid_under, probs_under)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_valid_under, Y_pred_under)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Метрики качества нашей модели\nprint_metrics(Y_valid_under, Y_pred_under)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"У новой модели гораздо лучше определяются результаты TN.\n\nПроверим ещё один способ работы с несбалансированными классами - метод oversampling.\n\nДля этого увеличим количество примеров меньшего класса примерно до количества примеров большего класса, просто скопровав их несколько раз.","metadata":{}},{"cell_type":"markdown","source":"# Модель с oversampling","metadata":{}},{"cell_type":"code","source":"# Сравняем количество классов в обучающей выборке, скопировав несколько раз примеры с default=1\ndata_over_1 = train_data[train_data['default'] == 1].copy()\ncount_1 = len(data_over_1)\ndata_over_0 = train_data[train_data['default'] == 0].copy()\ncount_0 = len(data_over_0)\n\nM = round(count_0 / count_1)\n\ndf_list = [data_over_1]\ndf_list = df_list*M\ndf_list.append(data_over_0)\n\ndata_over = pd.concat(df_list).reset_index()\n# Проверим количество классов\n\ndata_over['default'].value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Разделим датасет на обучающий и валидационный\nX_train_over, X_valid_over, Y_train_over, Y_valid_over = form(data_over)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучаем модель логистической регрессии\nmodel_over = LogisticRegression(solver='liblinear', max_iter=5000)\nmodel_over.fit(X_train_over, Y_train_over)\n\nprobs_over = model_over.predict_proba(X_valid_over)\nY_pred_over = model_over.predict(X_valid_over)\nprobs_over = probs_over[:,1]\n\n# Выводим на экран график roc_auc\nprint_roc_auc(Y_valid_over, probs_over)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_valid_over, Y_pred_over)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Метрики качества нашей модели\nprint_metrics(Y_valid_over, Y_pred_over)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Показатели этой модели совсем чуть-чуть отличаются от предыдущей модели.\n\nНесмотря на метрику ROC_AUC, другие метрики показали, что лучше определяют классы модели с undersampling и oversampling. Показатели у них примерно одинаковые, но oversampling требует значительно больше вычислительных ресурсов. Поэтому выбираем модель с undersampling как наилучшую.","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:55:02.482609Z","iopub.status.idle":"2021-11-29T17:55:02.483135Z","shell.execute_reply.started":"2021-11-29T17:55:02.482947Z","shell.execute_reply":"2021-11-29T17:55:02.482968Z"}}},{"cell_type":"markdown","source":"### Работа с выбранной моделью","metadata":{}},{"cell_type":"code","source":"# Проверим, не переобучилась ли наша модель\n# Для этого сначала посчитаем метрики модели для тренировочной выборки\nY_pred_train_under = model_under.predict(X_train_under)\nprint('Метрики модели на тренировочных данных')\nprint_metrics(Y_train_under, Y_pred_train_under)\n\n# А затем для для валидационной выборки\nprint('\\nМетрики модели на валидационных данных')\nY_pred_valid_under = model_under.predict(X_valid_under)\nprint_metrics(Y_valid_under, Y_pred_valid_under)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Метрики почти не отличаются, значит, модель не переобучилась ","metadata":{}},{"cell_type":"markdown","source":"Теперь с помощью функции GridSearchCV найдём оптимальные гиперпараметры для нашей модели и посмотрим, улучшатся ли метрики","metadata":{}},{"cell_type":"code","source":"# Выключим предупреждения\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodel = LogisticRegression()\n\niter_ = 50\nepsilon_stop = 1e-3\n\n# Создадим список гиперпараметров\nparam_grid = [\n    {'penalty': ['l1'], \n     'solver': ['liblinear', 'saga'], \n     'class_weight': [None, 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter': [iter_],\n     'tol': [epsilon_stop]},\n    \n    {'penalty': ['none'], \n     'solver': ['newton-cg', 'saga'], \n     'class_weight': [None, 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter': [iter_],\n     'tol': [epsilon_stop]},\n]\n\ngridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=12, cv=5)\ngridsearch.fit(X_train_under, Y_train_under)\nmodel = gridsearch.best_estimator_\n\n# Печатаем получившиеся наилучшие гиперпараметры\nprint('Наилучшие гиперпараметры:')\nbest_parameters = model.get_params()\nfor param_name in sorted(best_parameters.keys()):\n        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n        \n# Печатаем метрики с этими наилучшими гиперпараметрами\npreds = model.predict(X_valid_under)\ny_pred_prob = model.predict_proba(X_valid_under)[:,1]\ny_pred = model.predict(X_valid_under)\n\nprint('\\nПолучившиеся метрики:')\nprint_metrics(Y_valid_under, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видим, метрики практически не изменились. Тогда оставим гиперпараметры по умолчанию.","metadata":{}},{"cell_type":"code","source":"# Подготовим тестовые данные для работы с моделью\n# Удалим столбец default с фиктивными значениями\nX_test_pd = test_data.drop(['default'], axis=1)\n\n# Стандартизуем числовые данные по всем данным в тренировочном датасете\nscaler_best = StandardScaler()\nscaler_best.fit(train_data[num_cols].values)\nX_test_num = scaler_best.transform(X_test_pd[num_cols].values)\n\n# И соберём все признаки в numpy array\nX_test = np.hstack([X_test_num, X_test_pd.drop(columns=num_cols).values])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Создание файла submission","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_sub = model_under.predict_proba(X_test)\ntest['default'] = Y_sub[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test[['client_id','default']]\ndisplay(submission.head(10))\ndisplay(submission.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}